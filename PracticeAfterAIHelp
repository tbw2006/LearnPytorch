import torch
from torchvision import transforms,datasets
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
train_transform = transforms.Compose([
    transforms.RandomCrop(32,padding=4),#四周填充4个像素，再随机裁剪出32*32
    transforms.ToTensor(),
    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))

])

test_transform = transforms.Compose([
    transforms.Pad(4),  # 填充4像素，保持与训练一致
    transforms.CenterCrop(32),  # 确定性中心裁剪
    transforms.ToTensor(),
    transforms.Normalize((0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010))  # 添加归一化
])

train_dataset = datasets.CIFAR10(root='./CIFAR10',
    train=True,
    download=True,
    transform=train_transform)

test_dataset = datasets.CIFAR10(
    root='./CIFAR10',
    train=False,
    download=True,
    transform=test_transform
    )


batch_size = 64
train_loader = DataLoader(
    train_dataset,
    batch_size= batch_size,
    shuffle = True
    
    )

test_loader = DataLoader(
    test_dataset,
    batch_size = batch_size,
    shuffle = False
    )

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # 输入通道3，输出通道32
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)  # 下采样
        self.fc1 = nn.Linear(64 * 8 * 8, 256)  # 计算尺寸变化后的维度
        self.fc2 = nn.Linear(256, 10)
        self.dropout = nn.Dropout(0.5)  # 防止过拟合

    def forward(self, x):
        x = F.relu(self.conv1(x))    # 32x32x32
        x = self.pool(x)             # 32x16x16
        x = F.relu(self.conv2(x))    # 64x16x16
        x = self.pool(x)             # 64x8x8
        x = x.view(-1, 64 * 8 * 8)       # 展平
        x = self.dropout(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def evaluate(test_loader,model):
    model.eval()
    total = 0
    right = 0
    with torch.no_grad():
        for x,y in test_loader:
            outputs = model.forward(x)
            for i,output in enumerate(outputs):
                if torch.argmax(output) == y[i]:
                    right += 1
                total += 1
    return right/total


if __name__ == '__main__':
    model = Model() 
    
    optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)  # 当准确率不提升时降低学习率

    criterion = nn.CrossEntropyLoss()
    for epoch in range(5):
        model.train()
        for x , y in train_loader:
            model.zero_grad()
            outputs = model.forward(x)
            loss = criterion(outputs,y)
            loss.backward()
            optimizer.step()
        acc = evaluate(test_loader, model)
        scheduler.step(acc)  # 根据准确率调整学习率
        print("epoch", epoch, "accuracy", acc)
            


